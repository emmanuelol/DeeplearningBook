{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting .\\mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting .\\mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets(os.path.join('.', 'mnist'), one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=mnist.train.images\n",
    "X_test=mnist.test.images\n",
    "Y_train=mnist.train.labels\n",
    "Y_test=mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs=10\n",
    "num_inputs=784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x,num_inputs,num_outputs,num_layers,num_neurons):\n",
    "    w=[]\n",
    "    b=[]\n",
    "    for i in range(num_layers):\n",
    "        #weights\n",
    "        w.append(tf.Variable(tf.random_normal([num_inputs if i==0 else num_neurons[i-1],num_neurons[i]]),name='w_{0:04d}'.format(i)))\n",
    "        #biases\n",
    "        b.append(tf.Variable(tf.random_normal([num_neurons[i]]),name='b_{0:04d}'.format(i)))\n",
    "        \n",
    "    #weights last layer\n",
    "    w.append(tf.Variable(tf.random_normal([num_neurons[num_layers-1] if num_layers>0 else num_inputs,num_outputs]),name='w_out'))\n",
    "    #biases last layer\n",
    "    b.append(tf.Variable(tf.random_normal([num_outputs]),name='b_out'))\n",
    "        \n",
    "    #X is input layer\n",
    "    layer=x\n",
    "    #hidden layers\n",
    "    for i in range(num_layers):\n",
    "        layer=tf.nn.relu(tf.matmul(layer,w[i])+b[i])\n",
    "    #add output layer\n",
    "    layer=tf.matmul(layer,w[num_layers]+b[num_layers])\n",
    "    \n",
    "    return layer\n",
    "            \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_batch_func(batch_size=100):\n",
    "    X_batch, Y_batch=mnist.train.next_batch(batch_size)\n",
    "    return [X_batch,Y_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_classification(n_epochs, n_batches, batch_size, model, optimizer, loss, accuracy_function, X_test, Y_test):\n",
    "    with tf.Session() as tfs:\n",
    "        tfs.run(tf.global_variables_initializer())\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch in range(n_batches):\n",
    "                X_batch, Y_batch = mnist_batch_func(batch_size)\n",
    "                feed_dict = {x: X_batch, y: Y_batch}\n",
    "                _, batch_loss = tfs.run([optimizer, loss], feed_dict)\n",
    "                epoch_loss += batch_loss\n",
    "            average_loss = epoch_loss / n_batches\n",
    "            print(\"epoch: {0:04d} loss = {1:0.6f}\".format(epoch, average_loss))\n",
    "        feed_dict = {x: X_test, y: Y_test}\n",
    "        accuracy_score = tfs.run(accuracy_function, feed_dict=feed_dict)\n",
    "        print(\"accuracy={0:.8f}\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\",shape=[None, num_inputs]) \n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\",shape=[None, num_outputs])\n",
    "num_layers = 0\n",
    "num_neurons = []\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp(x=x,num_inputs=num_inputs,num_outputs=num_outputs,num_layers=num_layers,num_neurons=num_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "predictions_check = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000 loss = 16.623318\n",
      "epoch: 0001 loss = 10.610090\n",
      "epoch: 0002 loss = 7.206725\n",
      "epoch: 0003 loss = 5.227058\n",
      "epoch: 0004 loss = 4.129632\n",
      "epoch: 0005 loss = 3.447593\n",
      "epoch: 0006 loss = 2.902883\n",
      "epoch: 0007 loss = 2.562798\n",
      "epoch: 0008 loss = 2.277590\n",
      "epoch: 0009 loss = 2.135688\n",
      "epoch: 0010 loss = 2.038962\n",
      "epoch: 0011 loss = 1.884609\n",
      "epoch: 0012 loss = 1.798710\n",
      "epoch: 0013 loss = 1.673571\n",
      "epoch: 0014 loss = 1.644344\n",
      "epoch: 0015 loss = 1.549257\n",
      "epoch: 0016 loss = 1.552558\n",
      "epoch: 0017 loss = 1.471107\n",
      "epoch: 0018 loss = 1.442182\n",
      "epoch: 0019 loss = 1.404987\n",
      "epoch: 0020 loss = 1.369554\n",
      "epoch: 0021 loss = 1.365152\n",
      "epoch: 0022 loss = 1.307181\n",
      "epoch: 0023 loss = 1.262045\n",
      "epoch: 0024 loss = 1.247930\n",
      "epoch: 0025 loss = 1.216945\n",
      "epoch: 0026 loss = 1.226793\n",
      "epoch: 0027 loss = 1.217638\n",
      "epoch: 0028 loss = 1.184781\n",
      "epoch: 0029 loss = 1.145880\n",
      "epoch: 0030 loss = 1.161637\n",
      "epoch: 0031 loss = 1.149745\n",
      "epoch: 0032 loss = 1.124426\n",
      "epoch: 0033 loss = 1.124790\n",
      "epoch: 0034 loss = 1.129827\n",
      "epoch: 0035 loss = 1.092602\n",
      "epoch: 0036 loss = 1.075715\n",
      "epoch: 0037 loss = 1.100028\n",
      "epoch: 0038 loss = 1.031801\n",
      "epoch: 0039 loss = 1.053830\n",
      "epoch: 0040 loss = 1.062264\n",
      "epoch: 0041 loss = 1.034812\n",
      "epoch: 0042 loss = 1.052166\n",
      "epoch: 0043 loss = 1.007007\n",
      "epoch: 0044 loss = 1.032691\n",
      "epoch: 0045 loss = 0.998333\n",
      "epoch: 0046 loss = 1.017122\n",
      "epoch: 0047 loss = 1.013976\n",
      "epoch: 0048 loss = 0.958675\n",
      "epoch: 0049 loss = 0.990577\n",
      "accuracy=0.85399997\n"
     ]
    }
   ],
   "source": [
    "tensorflow_classification(n_epochs, n_batches, batch_size, model, optimizer, loss, accuracy_function, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
