{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ledra\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.layers import Input,Dense,Reshape,Dot,merge\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the jupyter buffers\n",
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETSLIB_HOME = os.path.join(os.path.expanduser('~'),'dl-ts','datasetslib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not DATASETSLIB_HOME in sys.path:\n",
    "    sys.path.append(DATASETSLIB_HOME)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import datasetslib\n",
    "from datasetslib import util as dsu\n",
    "from datasetslib import nputil\n",
    "datasetslib.datasets_root = os.path.join(os.path.expanduser('~'),'datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: C:\\Users\\ledra\\datasets\\ptb-simple\\simple-examples.tgz\n",
      "Train : [9970 9971 9972 9974 9975]\n",
      "Test:  [102  14  24  32 752]\n",
      "Valid:  [1132   93  358    5  329]\n",
      "Vocabulary Length =  10000\n"
     ]
    }
   ],
   "source": [
    "from datasetslib.ptb import PTBSimple\n",
    "ptb = PTBSimple()\n",
    "# downloads data, converts words to ids, converts files to a list of ids\n",
    "ptb.load_data()\n",
    "print('Train :', ptb.part['train'][0:5])\n",
    "print('Test: ', ptb.part['test'][0:5])\n",
    "print('Valid: ', ptb.part['valid'][0:5])\n",
    "print('Vocabulary Length = ', ptb.vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CBOW pairs : context,target\n",
      "( ['aer', 'banknote', 'calloway', 'centrust'] , 9972 berlitz )\n",
      "( ['banknote', 'berlitz', 'centrust', 'cluett'] , 9974 calloway )\n",
      "( ['berlitz', 'calloway', 'cluett', 'fromstein'] , 9975 centrust )\n",
      "( ['calloway', 'centrust', 'fromstein', 'gitano'] , 9976 cluett )\n",
      "( ['centrust', 'cluett', 'gitano', 'guterman'] , 9980 fromstein )\n",
      "( ['cluett', 'fromstein', 'guterman', 'hydro-quebec'] , 9981 gitano )\n",
      "( ['fromstein', 'gitano', 'hydro-quebec', 'ipo'] , 9982 guterman )\n",
      "( ['gitano', 'guterman', 'ipo', 'kia'] , 9983 hydro-quebec )\n",
      "( ['guterman', 'hydro-quebec', 'kia', 'memotec'] , 9984 ipo )\n",
      "( ['hydro-quebec', 'ipo', 'memotec', 'mlx'] , 9986 kia )\n"
     ]
    }
   ],
   "source": [
    "ptb.skip_window = 2\n",
    "ptb.reset_index()\n",
    "# in CBOW input is the context word and output is the target word\n",
    "y_batch, x_batch = ptb.next_batch_cbow()\n",
    "\n",
    "print('The CBOW pairs : context,target')\n",
    "for i in range(5 * ptb.skip_window):\n",
    "    print('(', [ptb.id2word[x_i] for x_i in x_batch[i]],\n",
    "          ',', y_batch[i], ptb.id2word[y_batch[i]], ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skip-gram pairs : target,context\n",
      "( 9972 berlitz , 9970 aer )\n",
      "( 9972 berlitz , 9971 banknote )\n",
      "( 9972 berlitz , 9974 calloway )\n",
      "( 9972 berlitz , 9975 centrust )\n",
      "( 9974 calloway , 9971 banknote )\n",
      "( 9974 calloway , 9972 berlitz )\n",
      "( 9974 calloway , 9975 centrust )\n",
      "( 9974 calloway , 9976 cluett )\n",
      "( 9975 centrust , 9972 berlitz )\n",
      "( 9975 centrust , 9974 calloway )\n"
     ]
    }
   ],
   "source": [
    "ptb.skip_window = 2\n",
    "ptb.reset_index()\n",
    "# in skip-gram input is the target word and output is the context word\n",
    "x_batch, y_batch = ptb.next_batch_sg()\n",
    "\n",
    "print('The skip-gram pairs : target,context')\n",
    "for i in range(5 * ptb.skip_window):\n",
    "    print('(', x_batch[i], ptb.id2word[x_batch[i]],\n",
    "          ',', y_batch[i], ptb.id2word[y_batch[i]], ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  [72 20 44 64 45 67  9  2]\n"
     ]
    }
   ],
   "source": [
    "valid_size = 8\n",
    "x_valid = np.random.choice(valid_size * 10, valid_size, replace=False)\n",
    "print('valid: ',x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1024\n",
    "embedding_size=512\n",
    "n_negative_samples=64\n",
    "ptb.skip_window=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_table=sequence.make_sampling_table(ptb.vocab_len)\n",
    "pairs,labels=sequence.skipgrams(ptb.part['train'],ptb.vocab_len,window_size=ptb.skip_window,sampling_table=sample_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the skip-gram pairs: target,context\n",
      "['1383option', '2<eos>'] : 1\n",
      "['27its', '315contract'] : 0\n",
      "['1313hands', '10that'] : 1\n",
      "['4689entering', '2750restaurants'] : 1\n",
      "['5045wellington', '7343corrected'] : 0\n",
      "['9173modify', '5902appetite'] : 0\n",
      "['8548widow', '6012earns'] : 1\n",
      "['10that', '25be'] : 1\n",
      "['936receive', '5to'] : 1\n",
      "['1852ground', '1280southern'] : 1\n"
     ]
    }
   ],
   "source": [
    "print('the skip-gram pairs: target,context' )\n",
    "for i in range(5*ptb.skip_window):\n",
    "    print(['{}{}'.format(id,ptb.id2word[id]) for id in pairs[i]],':',labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=zip(*pairs)\n",
    "x=np.array(x,dtype=np.int32)\n",
    "x=nputil.to2d(x,unit_axis=1)\n",
    "y=np.array(y,dtype=np.int32)\n",
    "y=nputil.to2d(y,unit_axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the target world\n",
    "target_in=Input(shape=(1,),name='target_in')\n",
    "target=Embedding(ptb.vocab_len,embedding_size,input_length=1,name='target_em')(target_in)\n",
    "target=Reshape((embedding_size,1),name='target_re')(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build context model\n",
    "context_in=Input((1,),name='context_in')\n",
    "context=Embedding(ptb.vocab_len,embedding_size,input_length=1,name='context_em')(context_in)\n",
    "context=Reshape((embedding_size,1),name='context_re')(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the models with the dot product to check for  similarity and add sigmoid layer\n",
    "output=Dot(axes=1,name='output_dot')([target,context])\n",
    "output=Reshape((1,),name='output_re')(output)\n",
    "output=Dense(1,activation='sigmoid',name='output_sig')(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the functional model for finding word vectors\n",
    "model=Model(inputs=[target_in,context_in],outputs=output)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the models and create model to check for cosine similarity\n",
    "similarity=Dot(axes=0,normalize=True,name='sim_dot')([target_in, context_in])\n",
    "similarity_model=Model(inputs=[target_in,context_in],outputs=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "target_in (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "context_in (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "target_em (Embedding)           (None, 1, 512)       5120000     target_in[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "context_em (Embedding)          (None, 1, 512)       5120000     context_in[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "target_re (Reshape)             (None, 512, 1)       0           target_em[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "context_re (Reshape)            (None, 512, 1)       0           context_em[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output_dot (Dot)                (None, 1, 1)         0           target_re[0][0]                  \n",
      "                                                                 context_re[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output_re (Reshape)             (None, 1)            0           output_dot[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output_sig (Dense)              (None, 1)            2           output_re[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,240,002\n",
      "Trainable params: 10,240,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1100024/1100024 [==============================] - 341s 310us/step - loss: 0.4455\n",
      "Epoch 2/5\n",
      "1100024/1100024 [==============================] - 277s 252us/step - loss: 0.2476\n",
      "Epoch 3/5\n",
      "1100024/1100024 [==============================] - 277s 252us/step - loss: 0.0675\n",
      "Epoch 4/5\n",
      "1100024/1100024 [==============================] - 278s 252us/step - loss: 0.0256\n",
      "Epoch 5/5\n",
      "1100024/1100024 [==============================] - 280s 254us/step - loss: 0.0217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab0f3ebe48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs=5\n",
    "batch_size=1024\n",
    "model.fit([x,y],labels,batch_size=batch_size,epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar to years: ipo, isi, kia, memotec, mlx,\n",
      "Similar to from: ipo, isi, kia, memotec, mlx,\n",
      "Similar to says: ipo, isi, kia, memotec, mlx,\n",
      "Similar to we: swapo, isi, kia, memotec, mlx,\n",
      "Similar to more: ipo, isi, kia, memotec, mlx,\n",
      "Similar to when: swapo, isi, kia, memotec, mlx,\n",
      "Similar to 's: ipo, isi, kia, memotec, mlx,\n",
      "Similar to <eos>: swapo, isi, kia, memotec, mlx,\n"
     ]
    }
   ],
   "source": [
    "# print closest words to validation set at end of training\n",
    "top_k = 5  \n",
    "y_val = np.arange(ptb.vocab_len, dtype=np.int32)\n",
    "y_val = nputil.to2d(y_val,unit_axis=1)\n",
    "for i in range(valid_size):\n",
    "    x_val = np.full(shape=(ptb.vocab_len,1),fill_value=x_valid[i], dtype=np.int32)\n",
    "    similarity_scores = similarity_model.predict([x_val,y_val])\n",
    "    similarity_scores=similarity_scores.flatten()\n",
    "    similar_words = (-similarity_scores).argsort()[1:top_k + 1]\n",
    "    similar_str = 'Similar to {0:}:'.format(ptb.id2word[x_valid[i]])\n",
    "    for k in range(top_k):\n",
    "        similar_str = '{0:} {1:},'.format(similar_str, ptb.id2word[similar_words[k]])\n",
    "    print(similar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
