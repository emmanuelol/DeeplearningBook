{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mezclando estilos diferentes de programaci√≥n usando helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "[(x_train, y_train), (x_test, y_test)]=tf.keras.datasets.cifar10.load_data ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes).astype(np.float32)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train: (50000, 32, 32, 3)\n",
      "y train: (50000, 10)\n",
      "x test: (10000, 32, 32, 3)\n",
      "y test: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('x train:',x_train.shape)\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "#pd.get_dummies(y_train)\n",
    "print('y train:',y_train.shape)\n",
    "num_labels=10\n",
    "print('x test:',x_test.shape)\n",
    "print('y test:',y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size= 32\n",
    "patch_size=5\n",
    "depth = 32\n",
    "depth2=64\n",
    "redu=8\n",
    "num_hidden = 4096\n",
    "num_hidden_2=1024\n",
    "num_channels=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial=tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial=tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#funcion helper convulucion con stride 1 y padding same\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "#Max pooling stride 2, kernel 2 y padding same\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,[1,2,2,1],[1,2,2,1],padding='SAME')\n",
    "\n",
    "# conv layer\n",
    "\n",
    "def conv_layer(input,shape):\n",
    "    W=weight_variable(shape)\n",
    "    b=bias_variable([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input,W)+b)\n",
    "#full layer \n",
    "def full_layer(input,size):\n",
    "    in_size=int(input.get_shape()[1])\n",
    "    W=weight_variable([in_size,size])\n",
    "    b=bias_variable([size])\n",
    "    return tf.matmul(input,W)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_test_dataset = tf.constant(x_test,dtype=tf.float32)\n",
    "  keep_prob=tf.placeholder(tf.float32)  \n",
    "\n",
    "  def model(data,keep_prob):\n",
    "        conv1=conv_layer(data,shape=[patch_size,patch_size,num_channels,depth])\n",
    "        conv1_pool=max_pool_2x2(conv1)\n",
    "        conv2=conv_layer(conv1_pool,shape=[patch_size,patch_size,depth,depth2])\n",
    "        conv2_pool=max_pool_2x2(conv2)\n",
    "        conv2_flat=tf.reshape(conv2_pool,[-1,redu*redu*depth2])\n",
    "        #fully connected\n",
    "        full_1=tf.nn.relu(full_layer(conv2_flat,num_hidden_2))\n",
    "        full_1_drop=tf.nn.dropout(full_1,keep_prob=keep_prob)\n",
    "        return full_layer(full_1_drop,10)\n",
    "  \n",
    "  logits = model(tf_train_dataset,keep_prob=0.5) \n",
    "  loss = tf.reduce_mean(\n",
    "  tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \n",
    "  learning_rate=1e-3\n",
    "  optimizer=tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset,keep_prob=1.0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 22.775911\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 50: 2.263086\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 100: 1.899450\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 150: 2.140332\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 200: 1.976562\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 250: 2.259469\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 300: 1.446151\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 350: 1.388528\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 400: 1.543568\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 450: 1.826297\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 500: 1.968922\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 550: 2.017456\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 600: 1.659150\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 650: 1.925183\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 700: 1.609171\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 750: 1.949434\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 800: 1.290848\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 850: 1.597027\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 900: 1.692031\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 950: 1.765090\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 1000: 2.091540\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 1050: 1.552559\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 1100: 1.584656\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 1150: 0.889108\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 1200: 1.402193\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 1250: 1.865952\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 1300: 1.771428\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1350: 1.258130\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 1400: 1.460725\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1450: 1.479798\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 1500: 1.024111\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 1550: 1.758984\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1600: 1.658608\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1650: 1.342866\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 1700: 1.938798\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1750: 1.252556\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 1800: 1.421817\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 1850: 1.254371\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 1900: 1.206371\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 1950: 1.609064\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 2000: 1.511164\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2050: 1.310467\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2100: 1.713244\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 2150: 1.339774\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 2200: 1.459635\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2250: 1.871924\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2300: 1.569888\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2350: 1.142349\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2400: 1.474293\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 2450: 1.297488\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2500: 1.605328\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2550: 1.120638\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 2600: 1.035557\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 2650: 1.647763\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 2700: 1.634302\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2750: 0.979236\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 2800: 1.597310\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 2850: 1.472206\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 2900: 1.074412\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 2950: 1.168575\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3000: 1.515803\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 3050: 1.296519\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3100: 1.032549\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 3150: 0.938906\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 3200: 1.422976\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3250: 1.267146\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3300: 0.807932\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 3350: 1.497519\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3400: 1.276259\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 3450: 1.604228\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 3500: 1.869094\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 3550: 1.003792\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 3600: 1.337134\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 3650: 0.807305\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 3700: 1.064813\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3750: 1.059946\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3800: 1.205493\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 3850: 1.005849\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3900: 1.609278\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 3950: 1.339914\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 4000: 1.219937\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4050: 0.872026\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4100: 1.132553\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 4150: 0.961816\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4200: 1.165304\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 4250: 1.185007\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 4300: 1.228371\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 4350: 0.946863\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 4400: 1.416659\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 4450: 0.707382\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 4500: 1.559999\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4550: 1.668692\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 4600: 1.140179\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 4650: 1.032296\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 4700: 1.429867\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 4750: 1.100792\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 4800: 1.069520\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 4850: 1.183182\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 4900: 0.719522\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 4950: 1.339924\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 5000: 0.927795\n",
      "Minibatch accuracy: 68.8%\n",
      "Test accuracy: 13.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "    batch_data = x_train[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,keep_prob:0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "     \n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelo diferente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size=3\n",
    "batch_size=32\n",
    "C1,C2,C3=30,50,80\n",
    "F1=500\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_test_dataset = tf.constant(x_test,dtype=tf.float32)\n",
    "  keep_prob=tf.placeholder(tf.float32)  \n",
    "  def model(data,keep_prob):\n",
    "        \n",
    "        \n",
    "        conv1_1=conv_layer(data,shape=[patch_size,patch_size,num_channels,C1])\n",
    "        conv1_2=conv_layer(conv1_1,shape=[patch_size,patch_size,C1,C1])\n",
    "        conv1_3=conv_layer(conv1_2,shape=[patch_size,patch_size,C1,C1])\n",
    "        conv1_pool=max_pool_2x2(conv1_3)\n",
    "        conv1_drop=tf.nn.dropout(conv1_pool,keep_prob)\n",
    "     \n",
    "        conv2_1=conv_layer(conv1_drop,shape=[patch_size,patch_size,C1,C2])\n",
    "        conv2_2=conv_layer(conv2_1,shape=[patch_size,patch_size,C2,C2])\n",
    "        conv2_3=conv_layer(conv2_2,shape=[patch_size,patch_size,C2,C2])\n",
    "        conv2_pool=max_pool_2x2(conv2_3)\n",
    "        conv2_drop=tf.nn.dropout(conv2_pool,keep_prob)    \n",
    "        \n",
    "        conv3_1=conv_layer(conv2_drop,shape=[patch_size,patch_size,C2,C3])\n",
    "        conv3_2=conv_layer(conv3_1,shape=[patch_size,patch_size,C3,C3])\n",
    "        conv3_3=conv_layer(conv3_2,shape=[patch_size,patch_size,C3,C3])\n",
    "        conv3_pool=tf.nn.max_pool(conv3_3,[1,8,8,1],[1,8,8,1],padding='SAME')\n",
    "        \n",
    "        #flat\n",
    "        conv3_flat=tf.reshape(conv3_pool,[-1,C3])\n",
    "        conv3_drop=tf.nn.dropout(conv3_flat,keep_prob)\n",
    "\n",
    "        #fully connected\n",
    "        full_1=tf.nn.relu(full_layer(conv3_drop,F1))\n",
    "        full_1_drop=tf.nn.dropout(full_1,keep_prob=keep_prob)\n",
    "        return full_layer(full_1_drop,10)\n",
    "  \n",
    "  logits = model(tf_train_dataset,keep_prob=0.5) \n",
    "  loss = tf.reduce_mean(\n",
    "  tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \n",
    "  learning_rate=1e-3\n",
    "  optimizer=tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset,keep_prob=1.0))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 67.822128\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 50: 2.335329\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 100: 2.381514\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 150: 2.260598\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 200: 2.410365\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 250: 2.279494\n",
      "Minibatch accuracy: 9.4%\n",
      "Minibatch loss at step 300: 2.299127\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 350: 2.290463\n",
      "Minibatch accuracy: 3.1%\n",
      "Minibatch loss at step 400: 2.297115\n",
      "Minibatch accuracy: 9.4%\n",
      "Minibatch loss at step 450: 2.304297\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 500: 2.318113\n",
      "Minibatch accuracy: 3.1%\n",
      "Minibatch loss at step 550: 2.284433\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 600: 2.287393\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 650: 2.322190\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 700: 2.312058\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 750: 2.273959\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 800: 2.316602\n",
      "Minibatch accuracy: 9.4%\n",
      "Minibatch loss at step 850: 2.272498\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 900: 2.337027\n",
      "Minibatch accuracy: 9.4%\n",
      "Minibatch loss at step 950: 2.305668\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 1000: 2.230870\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 1050: 2.203318\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 1100: 2.320297\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1150: 2.239812\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 1200: 2.263196\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 1250: 2.264392\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 1300: 2.170512\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 1350: 2.159517\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 1400: 2.253275\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 1450: 2.216013\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 1500: 2.246242\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1550: 2.156396\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 1600: 2.174290\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 1650: 2.191128\n",
      "Minibatch accuracy: 9.4%\n",
      "Minibatch loss at step 1700: 2.219803\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 1750: 2.072971\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 1800: 2.231091\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1850: 2.248281\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 1900: 2.168957\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 1950: 2.126679\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 2000: 2.050141\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 2050: 2.227517\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 2100: 2.165609\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 2150: 2.136742\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 2200: 2.029384\n",
      "Minibatch accuracy: 28.1%\n",
      "Minibatch loss at step 2250: 2.242988\n",
      "Minibatch accuracy: 3.1%\n",
      "Minibatch loss at step 2300: 2.207153\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 2350: 1.962974\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 2400: 2.100463\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 2450: 2.116235\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 2500: 1.974352\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 2550: 2.140337\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 2600: 2.117929\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 2650: 1.974000\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 2700: 1.973074\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 2750: 1.916390\n",
      "Minibatch accuracy: 28.1%\n",
      "Minibatch loss at step 2800: 2.148239\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 2850: 1.925819\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 2900: 2.123160\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 2950: 2.094340\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 3000: 1.898269\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 3050: 1.815624\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 3100: 1.899406\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 3150: 2.081236\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 3200: 1.700341\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 3250: 2.021355\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 3300: 1.874730\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 3350: 1.780141\n",
      "Minibatch accuracy: 28.1%\n",
      "Minibatch loss at step 3400: 1.851096\n",
      "Minibatch accuracy: 28.1%\n",
      "Minibatch loss at step 3450: 2.159739\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3500: 1.875013\n",
      "Minibatch accuracy: 28.1%\n",
      "Minibatch loss at step 3550: 1.969633\n",
      "Minibatch accuracy: 28.1%\n",
      "Minibatch loss at step 3600: 1.857800\n",
      "Minibatch accuracy: 28.1%\n",
      "Minibatch loss at step 3650: 2.294473\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 3700: 1.924692\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 3750: 1.937531\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 3800: 1.834258\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 3850: 2.001836\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3900: 2.161151\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 3950: 1.797430\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 4000: 1.774924\n",
      "Minibatch accuracy: 28.1%\n",
      "Minibatch loss at step 4050: 1.790161\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4100: 1.949135\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4150: 1.836962\n",
      "Minibatch accuracy: 15.6%\n",
      "Minibatch loss at step 4200: 1.811193\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 4250: 1.703635\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 4300: 1.690243\n",
      "Minibatch accuracy: 40.6%\n",
      "Minibatch loss at step 4350: 1.858575\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4400: 1.771490\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 4450: 1.536559\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4500: 1.692522\n",
      "Minibatch accuracy: 28.1%\n",
      "Minibatch loss at step 4550: 1.716424\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4600: 1.477427\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 4650: 1.872045\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4700: 1.895207\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4750: 1.539541\n",
      "Minibatch accuracy: 34.4%\n",
      "Minibatch loss at step 4800: 1.865945\n",
      "Minibatch accuracy: 21.9%\n",
      "Minibatch loss at step 4850: 1.856724\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4900: 1.821177\n",
      "Minibatch accuracy: 28.1%\n",
      "Minibatch loss at step 4950: 1.576469\n",
      "Minibatch accuracy: 28.1%\n",
      "Minibatch loss at step 5000: 1.592995\n",
      "Minibatch accuracy: 56.2%\n",
      "Test accuracy: 10.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "    batch_data = x_train[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,keep_prob:0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "     \n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
