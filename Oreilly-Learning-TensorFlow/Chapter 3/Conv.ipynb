{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ejercicio CIFAR10 con tensorflow 1.4 o superior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "[(x_train, y_train), (x_test, y_test)]=tf.keras.datasets.cifar10.load_data ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes).astype(np.float32)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "#pd.get_dummies(y_train)\n",
    "print(y_train.shape)\n",
    "num_labels=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size= 32\n",
    "patch_size=5\n",
    "depth = 32\n",
    "depth2=64\n",
    "\n",
    "num_hidden = 4096\n",
    "num_hidden_2=1024\n",
    "num_channels=3\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    " # tf_valid_dataset = tf.constant(valid_dataset)\n",
    "#  tf_test_dataset = tf.constant(x_test)\n",
    "  tf_test_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))  \n",
    "  # Variables.\n",
    "#5*5*3*32\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "#5*5*32*64\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "\n",
    "# fully connected layers\n",
    "#4096, 1024\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_hidden_2], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden_2]))\n",
    "#1024,10\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden_2, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "#keep prob\n",
    "  keep_prob=tf.placeholder(\"float\")\n",
    "\n",
    "\n",
    " \n",
    "  # Model.\n",
    "  def model(data,keeps_prob):\n",
    "        #conv 1\n",
    "        #32*32*3*32\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    \n",
    "    #pooling\n",
    "    #16*16*32*32\n",
    "    pool_1=tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='VALID')\n",
    "    \n",
    "    # Dropout\n",
    "    #pool_1_drop=tf.nn.dropout(pool_1,keep_prob)\n",
    "    \n",
    "    #conv 2\n",
    "    #16*16*32*64\n",
    "    conv = tf.nn.conv2d(pool_1, layer2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    \n",
    "    #pooling\n",
    "    #8*8*64*64\n",
    "    pool_2=tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='VALID')\n",
    "    # Dropout\n",
    "    #pool_2_drop=tf.nn.dropout(pool_2,keep_prob)\n",
    "    \n",
    "    #conv 3\n",
    "    #conv = tf.nn.conv2d(pool_2, layer3_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    #hidden = tf.nn.relu(conv + layer3_biases)\n",
    "    \n",
    "    #pooling\n",
    "\n",
    "    \n",
    "    # Fully connected\n",
    "    \n",
    "    shape = pool_2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    #hidden = tf.nn.relu(tf.matmul(hidden, layer5_weights) + layer5_biases)\n",
    "        \n",
    "    hidden=tf.nn.dropout(hidden,keep_prob)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "\n",
    "## same model without dropout for testing\n",
    "  # Model.\n",
    "  def model_test(data):\n",
    "        #conv 1\n",
    "        #32*32*3*32\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    \n",
    "    #pooling\n",
    "    #16*16*32*32\n",
    "    pool_1=tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='VALID')\n",
    "    \n",
    "    # Dropout\n",
    "    #pool_1_drop=tf.nn.dropout(pool_1,keep_prob)\n",
    "    \n",
    "    #conv 2\n",
    "    #16*16*32*64\n",
    "    conv = tf.nn.conv2d(pool_1, layer2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    \n",
    "    #pooling\n",
    "    #8*8*64*64\n",
    "    pool_2=tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='VALID')\n",
    "    # Dropout\n",
    "    #pool_2_drop=tf.nn.dropout(pool_2,keep_prob)\n",
    "    \n",
    "    #conv 3\n",
    "    #conv = tf.nn.conv2d(pool_2, layer3_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    #hidden = tf.nn.relu(conv + layer3_biases)\n",
    "    \n",
    "    #pooling\n",
    "\n",
    "    \n",
    "    # Fully connected\n",
    "    \n",
    "    shape = pool_2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    #hidden = tf.nn.relu(tf.matmul(hidden, layer5_weights) + layer5_biases)\n",
    "        \n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "\n",
    "   # Training computation.\n",
    "  logits = model(tf_train_dataset,keep_prob)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  # regulizer\n",
    "  #regularizers=tf.nn.l2_loss(layer1_weights)+tf.nn.l2_loss(layer2_weights)+tf.nn.l2_loss(layer3_weights)+tf.nn.l2_loss(layer4_weights)+tf.nn.l2_loss(layer5_weights)\n",
    "  #loss=tf.reduce_mean(loss+beta*regularizers)\n",
    "  loss=tf.reduce_mean(loss)\n",
    "  # Optimizer.\n",
    "  # agregando un learning rate descendiente\n",
    "  #global_step=tf.Variable(0)\n",
    "  #start_learning_rate=0.5\n",
    "  #learning_rate=tf.train.exponential_decay(start_learning_rate,global_step,100000,0.96,staircase=True)\n",
    "  learning_rate=1e-4  \n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "  optimizer=tf.train.AdamOptimizer(learning_rate).minimize(loss) \n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  #valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    \n",
    "    \n",
    " \n",
    "  test_prediction = tf.nn.softmax(model_test(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 40.869518\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 50: 17.223484\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 100: 8.884357\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 150: 5.414640\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 200: 4.109779\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 250: 2.481517\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 300: 1.894425\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 350: 1.805705\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 400: 1.757953\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 450: 2.228845\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 500: 2.195414\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 550: 2.183071\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 600: 2.044859\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 650: 2.059647\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 700: 2.079650\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 750: 2.267914\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 800: 2.111121\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 850: 1.936912\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 900: 2.239705\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 950: 2.115005\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1000: 2.393015\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 1050: 2.208769\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 1100: 2.281788\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1150: 2.037380\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 1200: 2.368659\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1250: 2.283237\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 1300: 2.105520\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1350: 2.313376\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 1400: 2.085522\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1450: 2.287622\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 1500: 2.054984\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1550: 2.096783\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1600: 2.469709\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1650: 2.240946\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 1700: 2.341863\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 1750: 2.374244\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1800: 2.123943\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 1850: 2.045156\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1900: 1.970693\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1950: 2.047150\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 2000: 1.900059\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 2050: 2.051449\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 2100: 2.444692\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 2150: 2.075338\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 2200: 1.965988\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 2250: 1.909567\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2300: 2.056042\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 2350: 1.918511\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 2400: 1.978527\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 2450: 1.930656\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2500: 1.730433\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2550: 2.117031\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 2600: 2.221372\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 2650: 1.710426\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2700: 1.869836\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2750: 1.839234\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 2800: 1.715589\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 2850: 1.950235\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2900: 1.830482\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 2950: 1.957379\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 3000: 2.058672\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 3050: 2.218949\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3100: 1.932867\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 3150: 1.927374\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3200: 1.890445\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 3250: 2.003332\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3300: 1.820493\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 3350: 2.488851\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 3400: 2.208133\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 3450: 1.851600\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3500: 2.084866\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 3550: 1.846123\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 3600: 2.238612\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 3650: 1.721020\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 3700: 2.017025\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3750: 1.927245\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 3800: 1.727761\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 3850: 1.827060\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3900: 1.999542\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 3950: 2.141195\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4000: 1.769929\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4050: 1.836701\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 4100: 1.601831\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4150: 1.341581\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 4200: 1.813668\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 4250: 2.022502\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 4300: 1.938343\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4350: 1.859328\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 4400: 2.043877\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4450: 1.508300\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4500: 1.852680\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4550: 1.957263\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4600: 1.718024\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 4650: 1.886990\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4700: 1.866540\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4750: 1.666405\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4800: 2.093593\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 4850: 1.706975\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4900: 2.203008\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4950: 2.173126\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 5000: 1.488401\n",
      "Minibatch accuracy: 62.5%\n",
      "Test accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ledra\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "    batch_data = x_train[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,keep_prob:0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      #print('Validation accuracy: %.1f%%' % accuracy(\n",
    "       # valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
