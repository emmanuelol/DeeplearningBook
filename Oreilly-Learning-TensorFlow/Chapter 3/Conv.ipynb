{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ejercicio CIFAR10 con tensorflow 1.4 o superior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "[(x_train, y_train), (x_test, y_test)]=tf.keras.datasets.cifar10.load_data ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes).astype(np.float32)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train: (50000, 32, 32, 3)\n",
      "y train: (50000, 10)\n",
      "x test: (10000, 32, 32, 3)\n",
      "y test: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('x train:',x_train.shape)\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "#pd.get_dummies(y_train)\n",
    "print('y train:',y_train.shape)\n",
    "num_labels=10\n",
    "print('x test:',x_test.shape)\n",
    "print('y test:',y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size= 32\n",
    "patch_size=5\n",
    "depth = 32\n",
    "depth2=64\n",
    "\n",
    "num_hidden = 4096\n",
    "num_hidden_2=1024\n",
    "num_channels=3\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    " # tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(x_test,dtype=tf.float32)\n",
    "  #tf_test_dataset = tf.placeholder(\n",
    "   # tf.float32, shape=(batch_size, image_size, image_size, num_channels))  \n",
    "  # Variables.\n",
    "#5*5*3*32\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "#5*5*32*64\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "\n",
    "# fully connected layers\n",
    "#4096, 1024\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_hidden_2], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden_2]))\n",
    "#1024,10\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden_2, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "#keep prob\n",
    "  keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    " \n",
    "  # Model.\n",
    "  def model(data,keep_prob):\n",
    "   #def model(data,keep_prob):\n",
    "        #conv 1\n",
    "        #32*32*3*32\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    \n",
    "    #pooling\n",
    "    #16*16*32*32\n",
    "    pool_1=tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='SAME')\n",
    "    \n",
    "    # Dropout\n",
    "    #pool_1_drop=tf.nn.dropout(pool_1,keep_prob)\n",
    "    \n",
    "    #conv 2\n",
    "    #16*16*32*64\n",
    "    conv = tf.nn.conv2d(pool_1, layer2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    \n",
    "    #pooling\n",
    "    #8*8*64*64\n",
    "    pool_2=tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='SAME')\n",
    "    # Dropout\n",
    "    #pool_2_drop=tf.nn.dropout(pool_2,keep_prob)\n",
    "    \n",
    "    #conv 3\n",
    "    #conv = tf.nn.conv2d(pool_2, layer3_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    #hidden = tf.nn.relu(conv + layer3_biases)\n",
    "    \n",
    "    #pooling\n",
    "\n",
    "    \n",
    "    # Fully connected\n",
    "    \n",
    "    shape = pool_2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    #hidden = tf.nn.relu(tf.matmul(hidden, layer5_weights) + layer5_biases)\n",
    "        \n",
    "    hidden=tf.nn.dropout(hidden,keep_prob)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "\n",
    "## same model without dropout for testing\n",
    "  # Model.\n",
    " \n",
    "\n",
    "   # Training computation.\n",
    "  #logits = model(tf_train_dataset)\n",
    "  logits = model(tf_train_dataset,keep_prob=0.5) \n",
    "  loss = tf.reduce_mean(\n",
    "  tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  # regulizer\n",
    "  #regularizers=tf.nn.l2_loss(layer1_weights)+tf.nn.l2_loss(layer2_weights)+tf.nn.l2_loss(layer3_weights)+tf.nn.l2_loss(layer4_weights)+tf.nn.l2_loss(layer5_weights)\n",
    "  #loss=tf.reduce_mean(loss+beta*regularizers)\n",
    "  #loss=tf.reduce_mean(loss)\n",
    "  # Optimizer.\n",
    "  # agregando un learning rate descendiente\n",
    "  #global_step=tf.Variable(0)\n",
    "  #start_learning_rate=0.5\n",
    "  #learning_rate=tf.train.exponential_decay(start_learning_rate,global_step,100000,0.96,staircase=True)\n",
    "  learning_rate=1e-3  \n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "  optimizer=tf.train.AdamOptimizer(learning_rate).minimize(loss) \n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  #valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  #test_prediction = tf.nn.softmax(model(tf_test_dataset))    \n",
    "    \n",
    " \n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset,keep_prob=1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 33.536911\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 50: 5.111912\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 100: 3.651460\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 150: 3.992448\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 200: 4.229755\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 250: 2.187902\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 300: 3.187607\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 350: 1.441424\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 400: 1.911334\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 450: 2.201990\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 500: 2.018458\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 550: 1.978348\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 600: 1.941598\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 650: 2.154346\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 700: 1.757343\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 750: 1.837918\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 800: 1.726928\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 850: 1.681782\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 900: 1.884583\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 950: 1.949960\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 1000: 1.996446\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 1050: 1.868095\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 1100: 2.164083\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1150: 1.554167\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1200: 1.699514\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 1250: 1.967197\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1300: 1.942569\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1350: 1.472555\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 1400: 1.886827\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1450: 1.942657\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 1500: 1.253377\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 1550: 1.937760\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 1600: 1.953988\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 1650: 1.693121\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 1700: 1.886400\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 1750: 1.754497\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1800: 1.971105\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 1850: 1.952403\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 1900: 1.749527\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 1950: 1.833424\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 2000: 1.845833\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2050: 1.693641\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2100: 2.330405\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 2150: 1.564147\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2200: 1.717885\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 2250: 2.158876\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 2300: 1.825815\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2350: 1.300915\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2400: 1.516059\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 2450: 1.088932\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 2500: 1.524315\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 2550: 1.667362\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2600: 1.426213\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2650: 1.379316\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 2700: 1.645904\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 2750: 1.242636\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 2800: 1.486806\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 2850: 1.878033\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 2900: 1.524442\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 2950: 2.076306\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3000: 2.135897\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3050: 1.815361\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 3100: 1.541344\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3150: 1.121336\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 3200: 1.480596\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 3250: 1.658182\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3300: 1.005116\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 3350: 1.645129\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3400: 1.578288\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 3450: 1.662875\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 3500: 1.778967\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 3550: 1.161174\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 3600: 1.862632\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3650: 1.116545\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 3700: 1.462144\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 3750: 1.564160\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 3800: 1.430630\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 3850: 1.551494\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 3900: 1.462939\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 3950: 1.590105\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 1.658648\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 4050: 1.298305\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 4100: 1.095147\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4150: 1.198815\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 4200: 1.095988\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 4250: 1.347206\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4300: 1.568795\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 4350: 1.029389\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 4400: 1.848919\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 4450: 1.155694\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4500: 1.831785\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 4550: 1.860605\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 4600: 1.011351\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4650: 1.568939\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 4700: 1.846181\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 4750: 1.434472\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 4800: 0.952905\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 4850: 1.697167\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 4900: 1.316001\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 4950: 1.108193\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 5000: 1.255263\n",
      "Minibatch accuracy: 62.5%\n",
      "Test accuracy: 50.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "    batch_data = x_train[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,keep_prob:0.5}\n",
    "    #feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      #print('Validation accuracy: %.1f%%' % accuracy(\n",
    "       # valid_prediction.eval(), valid_labels))\n",
    "        \n",
    "  \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problema con otro estilo de programacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial=tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial=tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#funcion helper convulucion con stride 1 y padding same\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "#Max pooling stride 2, kernel 2 y padding same\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,[1,2,2,1],[1,2,2,1],padding='SAME')\n",
    "\n",
    "# conv layer\n",
    "\n",
    "def conv_layer(input,shape):\n",
    "    W=weight_variable(shape)\n",
    "    b=bias_variable([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input,W)+b)\n",
    "#full layer \n",
    "def full_layer(input,size):\n",
    "    in_size=int(input.get_shape()[1])\n",
    "    W=weight_variable([in_size,size])\n",
    "    b=bias_variable([size])\n",
    "    return tf.matmul(input,W)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entradas\n",
    "x=tf.placeholder(tf.float32,shape=[None,32,32,3])\n",
    "y=tf.placeholder(tf.float32,shape=[None,10])\n",
    "keep_prob=tf.placeholder(tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \n",
    "\n",
    "num_channels=3\n",
    "patch_size=5\n",
    "depth=32\n",
    "depth2=64\n",
    "num_hidden=1024\n",
    "batch_size=16\n",
    "\n",
    "conv1=conv_layer(x,shape=[patch_size,patch_size,num_channels,depth])\n",
    "conv1_pool=max_pool_2x2(conv1)\n",
    "\n",
    "conv2=conv_layer(conv1_pool,shape=[patch_size,patch_size,depth,depth2])\n",
    "conv2_pool=max_pool_2x2(conv2)\n",
    "conv2_flat=tf.reshape(conv2_pool,[-1,8*8*depth2])\n",
    "\n",
    "#fully connected\n",
    "full_1=tf.nn.relu(full_layer(conv2_flat,num_hidden))\n",
    "full_1_drop=tf.nn.dropout(full_1,keep_prob=keep_prob)\n",
    "\n",
    "y_conv=full_layer(full_1_drop,10)\n",
    "\n",
    "# salida\n",
    "cross_entropy= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_conv))\n",
    "\n",
    "train_step=tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "correct_prediction=tf.equal(tf.argmax(y_conv,1),tf.argmax(y,1))\n",
    "accuracy_model=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "train_prediction=tf.nn.softmax(y_conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def test(sess):\n",
    " #   x_test=x_test\n",
    "  #  y_test=y_test\n",
    "   # acc=np.mean([sess.run(accuracy,feed_dict={x:x_test[i],y:y_test[i],keep_prob=1}) for i in range(10)])\n",
    "    #print(\"Accuracy:{:.4}\".format(acc*100)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 16.729513\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 50: 2.324462\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 100: 1.842644\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 150: 2.045403\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 200: 2.259174\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 250: 1.991985\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 300: 1.627606\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 350: 1.518497\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 400: 1.879521\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 450: 1.573216\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 500: 1.971918\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 550: 1.567602\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 600: 1.941336\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 650: 2.083199\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 700: 1.505296\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 750: 1.886604\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 800: 1.486236\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 850: 1.783434\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 900: 1.648963\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 950: 1.884201\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 1000: 1.991778\n",
      "Minibatch accuracy: 18.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session() as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "    batch_data = x_train[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "    feed_dict = {x : batch_data, y : batch_labels,keep_prob:0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [train_step, cross_entropy, train_prediction], feed_dict=feed_dict)\n",
    "    #session.run(train_step,feed_dict = {x : batch_data, y : batch_labels,keep_prob:0.5})\n",
    "    \n",
    "      #[optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
